{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d19be70-74d6-4596-ba08-6df78588b83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Set the bucket name and document name based on your S3 URL\n",
    "bucket_name = 'textract-console-us-east-2-473b0b2a-4bbb-4f45-ae6e-997765bde30c'\n",
    "\n",
    "# Set directory for output\n",
    "folder = \"/Users/beaubressler/Library/CloudStorage/Dropbox/Research/urban_assistance_program/\"\n",
    "\n",
    "# !! Set document name: This is the document you want to read in from AWS\n",
    "document_name = 'project_directory_appendix_1966v2.pdf' \n",
    "\n",
    "# !! set output name: This is the name of the output csv you will produce\n",
    "output_csv = folder + \"data/digitization/intermediate/\" + \"project_directory_1966_appendix_a_textract.csv\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd5d480c-496e-41b9-bdf1-09e1a6d11a06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job started with ID: 8b12c8e6e6994a129017144eeeb00d53cca5c49add0044ab75899a2c518840ce\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: IN_PROGRESS\n",
      "Job status: SUCCEEDED\n",
      "Fetched page with 1000 blocks.\n",
      "Fetched page with 1000 blocks.\n",
      "Fetched page with 1000 blocks.\n",
      "Fetched page with 1000 blocks.\n",
      "Fetched page with 1000 blocks.\n",
      "Fetched page with 1000 blocks.\n",
      "Fetched page with 1000 blocks.\n",
      "Fetched page with 1000 blocks.\n",
      "Fetched page with 1000 blocks.\n",
      "Fetched page with 1000 blocks.\n",
      "Fetched page with 1000 blocks.\n",
      "Fetched page with 1000 blocks.\n",
      "Fetched page with 1000 blocks.\n",
      "Fetched page with 1000 blocks.\n",
      "Fetched page with 1000 blocks.\n",
      "Fetched page with 1000 blocks.\n",
      "Fetched page with 1000 blocks.\n",
      "Fetched page with 1000 blocks.\n",
      "Fetched page with 1000 blocks.\n",
      "Fetched page with 1000 blocks.\n",
      "Fetched page with 1000 blocks.\n",
      "Fetched page with 1000 blocks.\n",
      "Fetched page with 1000 blocks.\n",
      "Fetched page with 1000 blocks.\n",
      "Fetched page with 1000 blocks.\n",
      "Fetched page with 1000 blocks.\n",
      "Fetched page with 1000 blocks.\n",
      "Fetched page with 1000 blocks.\n",
      "Fetched page with 1000 blocks.\n",
      "Fetched page with 1000 blocks.\n",
      "Fetched page with 1000 blocks.\n",
      "Fetched page with 1000 blocks.\n",
      "Fetched page with 1000 blocks.\n",
      "Fetched page with 1000 blocks.\n",
      "Fetched page with 1000 blocks.\n",
      "Fetched page with 1000 blocks.\n",
      "Fetched page with 1000 blocks.\n",
      "Fetched page with 1000 blocks.\n",
      "Fetched page with 1000 blocks.\n",
      "Fetched page with 1000 blocks.\n",
      "Fetched page with 1000 blocks.\n",
      "Fetched page with 1000 blocks.\n",
      "Fetched page with 1000 blocks.\n",
      "Fetched page with 1000 blocks.\n",
      "Fetched page with 1000 blocks.\n",
      "Fetched page with 1000 blocks.\n",
      "Total blocks retrieved: 46824\n",
      "Extracted 43 table(s) from the document.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script uses Amazon Textract to analyze a document stored in an S3 bucket.\n",
    "It retrieves tables from the document, handles large documents with asynchronous \n",
    "processing and pagination, and outputs the table data as a structured set of blocks \n",
    "for further analysis or DataFrame conversion.\n",
    "\n",
    "### Workflow Steps:\n",
    "\n",
    "1. **Setup and Start Document Analysis**:\n",
    "    - Initialize Textract client and set the target S3 bucket and document name.\n",
    "    - `client.start_document_analysis()` starts the document analysis job in Textract.\n",
    "    - The job is launched asynchronously, meaning it runs in the background, which \n",
    "      is useful for large files. The job's ID is stored to track progress.\n",
    "\n",
    "2. **Monitoring Job Status**:\n",
    "    - The script enters a loop to repeatedly check the status of the job using\n",
    "      `client.get_document_analysis()`.\n",
    "    - Every 5 seconds, the script checks if the job is complete.\n",
    "    - The loop exits once the job status is `\"SUCCEEDED\"`, allowing the script to \n",
    "      move to retrieving the results.\n",
    "\n",
    "3. **Retrieving Paginated Results**:\n",
    "    - Textract returns results in pages (pagination) when dealing with large files, \n",
    "      so a `next_token` is used to retrieve all pages one by one.\n",
    "    - Each page is fetched using `client.get_document_analysis()`, with `next_token` \n",
    "      identifying where the next page starts.\n",
    "    - The response data for each page is stored in a `blocks` list, which holds \n",
    "      the structure of tables and other data from the document.\n",
    "\n",
    "4. **Filtering and Identifying Tables**:\n",
    "    - Once all pages are retrieved, the script filters `blocks` to identify \n",
    "      only those blocks where `BlockType` is `\"TABLE\"`.\n",
    "    - A count of the tables is printed for quick reference, and this data can be \n",
    "      further processed, converted to a DataFrame, or saved as a CSV as needed.\n",
    "\n",
    "5. **Example Output**:\n",
    "    - The blocks can then be processed and structured as needed, but this script \n",
    "      provides a foundation by fetching, identifying, and preparing the table data.\n",
    "\n",
    "### Example Variables:\n",
    "- `bucket_name`: S3 bucket where the document is stored.\n",
    "- `document_name`: The document name in the S3 bucket to analyze.\n",
    "- `folder` and `output_csv`: Local storage paths where results could be saved.\n",
    "\n",
    "### Error Handling:\n",
    "    - If the job status is not `\"SUCCEEDED\"`, an error message is printed to \n",
    "      indicate the job failed.\n",
    "\n",
    "Example usage:\n",
    "    Adjust `bucket_name`, `document_name`, and `folder` as needed, then run the \n",
    "    script to retrieve table data from the specified document.\n",
    "\"\"\"\n",
    "\n",
    "# Initialize Textract client\n",
    "client = boto3.client('textract', region_name='us-east-2')\n",
    "\n",
    "# Start document analysis (asynchronously for large files)\n",
    "response = client.start_document_analysis(\n",
    "    DocumentLocation={'S3Object': {'Bucket': bucket_name, 'Name': document_name}},\n",
    "    FeatureTypes=[\"TABLES\"]\n",
    ")\n",
    "\n",
    "job_id = response['JobId']\n",
    "print(f\"Job started with ID: {job_id}\")\n",
    "\n",
    "# Check the status of the job\n",
    "status = \"IN_PROGRESS\"\n",
    "while status == \"IN_PROGRESS\":\n",
    "    response = client.get_document_analysis(JobId=job_id)\n",
    "    status = response['JobStatus']\n",
    "    print(f\"Job status: {status}\")\n",
    "    if status == \"IN_PROGRESS\":\n",
    "        time.sleep(5)\n",
    "\n",
    "# Retrieve paginated results once job is complete\n",
    "if status == \"SUCCEEDED\":\n",
    "    blocks = []\n",
    "    next_token = None\n",
    "    \n",
    "    while True:\n",
    "        # Fetch results with pagination\n",
    "        if next_token:\n",
    "            response = client.get_document_analysis(JobId=job_id, NextToken=next_token)\n",
    "        else:\n",
    "            response = client.get_document_analysis(JobId=job_id)\n",
    "        \n",
    "        # Add blocks to our list\n",
    "        blocks.extend(response['Blocks'])\n",
    "        \n",
    "        # Check if there is another page of results\n",
    "        next_token = response.get('NextToken')\n",
    "        if not next_token:\n",
    "            break  # Exit loop if no more pages\n",
    "\n",
    "        print(f\"Fetched page with {len(response['Blocks'])} blocks.\")\n",
    "\n",
    "    print(f\"Total blocks retrieved: {len(blocks)}\")\n",
    "\n",
    "    # Process blocks to extract table data\n",
    "    tables = [block for block in blocks if block['BlockType'] == 'TABLE']\n",
    "    print(f\"Extracted {len(tables)} table(s) from the document.\")\n",
    "\n",
    "    # (Optional) Process each table block and save to a DataFrame or CSV as needed\n",
    "\n",
    "else:\n",
    "    print(f\"Job failed with status: {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9c79f48-b36b-44d1-aecf-1f155035777c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code takes JSON-formatted output from Amazon Textract, which organizes tables\n",
    "as a series of blocks, and converts it into a structured pandas DataFrame. The steps are:\n",
    "    \n",
    "1. **Mapping Rows and Columns** (`get_rows_columns_map`):\n",
    "    - This function organizes each table's data into rows and columns.\n",
    "    - `table_result` holds raw table data, while `blocks_map` is a dictionary \n",
    "      for quick lookup of each block by its unique ID.\n",
    "    - It finds each cell in the table by examining \"child\" relationships, retrieves \n",
    "      the row and column positions, and organizes cell content in a dictionary \n",
    "      using row and column indices.\n",
    "\n",
    "2. **Extracting Text from Each Cell** (`get_text`):\n",
    "    - Since each cell can contain multiple small blocks (like individual words),\n",
    "      this function gathers and joins all text in each cell.\n",
    "    - It loops through child blocks within a cell, collects each word, and joins them \n",
    "      into a complete sentence to produce the full text for each cell.\n",
    "\n",
    "3. **Creating a DataFrame for Each Table** (`generate_table_df`):\n",
    "    - Once rows and columns are mapped, this function converts them into a \n",
    "      structured pandas DataFrame.\n",
    "    - It arranges each row in the correct column order, filling in any missing \n",
    "      cells with empty strings, so the table structure is maintained.\n",
    "    - The output is a DataFrame with rows and columns correctly aligned.\n",
    "\n",
    "4. **Processing All Tables and Saving as CSV** (`process_textract_tables`):\n",
    "    - This main function manages the full document processing:\n",
    "        - Maps all blocks by their IDs for efficient lookup.\n",
    "        - Filters out blocks that represent tables only.\n",
    "        - Converts each table into a DataFrame using the helper functions above, \n",
    "          combines all tables, and saves the final result as a CSV file.\n",
    "    - The final output is a CSV file containing all tables from the document,\n",
    "      formatted for easy analysis.\n",
    "\n",
    "Example usage:\n",
    "    Assuming `blocks` is the list of blocks returned from Textract:\n",
    "    output_csv = \"textract_output.csv\"\n",
    "    process_textract_tables(blocks, output_csv)\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Function to map rows and columns for a single table\n",
    "def get_rows_columns_map(table_result, blocks_map):\n",
    "    rows = {}\n",
    "    for relationship in table_result['Relationships']:\n",
    "        if relationship['Type'] == 'CHILD':\n",
    "            for child_id in relationship['Ids']:\n",
    "                cell = blocks_map.get(child_id)\n",
    "                if cell and cell['BlockType'] == 'CELL':\n",
    "                    row_index = cell['RowIndex']\n",
    "                    col_index = cell['ColumnIndex']\n",
    "                    if row_index not in rows:\n",
    "                        rows[row_index] = {}\n",
    "                    rows[row_index][col_index] = get_text(cell, blocks_map)\n",
    "    return rows\n",
    "\n",
    "# Function to extract text from each cell block\n",
    "def get_text(result, blocks_map):\n",
    "    text = ''\n",
    "    if 'Relationships' in result:\n",
    "        for relationship in result['Relationships']:\n",
    "            if relationship['Type'] == 'CHILD':\n",
    "                for child_id in relationship['Ids']:\n",
    "                    word = blocks_map.get(child_id)\n",
    "                    if word and word['BlockType'] == 'WORD':\n",
    "                        text += word['Text'] + ' '\n",
    "    return text.strip()\n",
    "\n",
    "# Function to convert the extracted table data into a DataFrame\n",
    "def generate_table_df(table_result, blocks_map):\n",
    "    rows = get_rows_columns_map(table_result, blocks_map)\n",
    "    table_data = []\n",
    "    for row_index in sorted(rows.keys()):\n",
    "        row = [rows[row_index].get(col, '') for col in sorted(rows[row_index].keys())]\n",
    "        table_data.append(row)\n",
    "    df = pd.DataFrame(table_data)\n",
    "    return df\n",
    "\n",
    "# Main processing function after pagination\n",
    "def process_textract_tables(blocks, output_csv):\n",
    "    # Map blocks for quick lookup\n",
    "    blocks_map = {block['Id']: block for block in blocks}\n",
    "    tables = [block for block in blocks if block['BlockType'] == 'TABLE']\n",
    "    \n",
    "    # Generate DataFrames for each table and concatenate\n",
    "    table_dfs = [generate_table_df(table, blocks_map) for table in tables]\n",
    "    final_df = pd.concat(table_dfs, ignore_index=True)\n",
    "\n",
    "    # Print the final DataFrame and save to CSV\n",
    "    print(\"Final DataFrame:\\n\", final_df)\n",
    "    final_df.to_csv(output_csv, index=False)\n",
    "    print(f\"Data saved to {output_csv}\")\n",
    "\n",
    "# Example usage:\n",
    "# Assuming `blocks` is the full list of blocks retrieved from Textract\n",
    "# output_csv = \"textract_output.csv\"\n",
    "# process_textract_tables(blocks, output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2a6644e-ae4a-493c-a29e-33dcbbf27694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final DataFrame:\n",
      "                0                 1               2               3          4  \\\n",
      "0          Place            County  Project no(s).           Place     County   \n",
      "1        ALABAMA                                    ALABAMA (Con.)              \n",
      "2      Abbeville             Henry           P-3,4       Fairfield  Jefferson   \n",
      "3     Adamsville         Jefferson           P-6,8        Fairhope    Baldwin   \n",
      "4        Addison           Winston            P-10         Fayette    Fayette   \n",
      "...          ...               ...             ...             ...        ...   \n",
      "2559      Verona              Dane            P-20                              \n",
      "2560       Waldo         Sheboygan            P-20                              \n",
      "2561    Walworth          Walworth            P-15                              \n",
      "2562    Waterloo         Jefferson            P-37                              \n",
      "2563   Watertown  Dodge, Jefferson             P-1                              \n",
      "\n",
      "                   5  \n",
      "0     Project no(s).  \n",
      "1                     \n",
      "2            P-2,3,4  \n",
      "3                P-1  \n",
      "4      P-3,4,8,10,11  \n",
      "...              ...  \n",
      "2559                  \n",
      "2560                  \n",
      "2561                  \n",
      "2562                  \n",
      "2563                  \n",
      "\n",
      "[2564 rows x 6 columns]\n",
      "Data saved to /Users/beaubressler/Library/CloudStorage/Dropbox/Research/urban_assistance_program/data/digitization/intermediate/project_directory_1966_appendix_a_textract.csv\n"
     ]
    }
   ],
   "source": [
    "process_textract_tables(blocks, output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a558f0c-961d-4a01-977a-2421312c2dd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
